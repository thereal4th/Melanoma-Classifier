{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBEOnc1yg55y"
   },
   "outputs": [],
   "source": [
    "# Potential issues:\n",
    "# No Segmentation ‚Üí Model May Learn Irrelevant Features\n",
    "# Learning Rate Might Not Be Optimal (higher if not learning, lower if overfitting)\n",
    "# Image Resolution Might Be Too Low (128x128)\n",
    "# Batch Size (32) Might Be Suboptimal (try 16 or 64)\n",
    "# If you didn‚Äôt use flipping, rotation, brightness changes, or color jittering, your model might overfit to training images and fail on new ones.\n",
    "#\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Setup device-agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "\n",
    "batchSize = 32\n",
    "imageResolution = 224\n",
    "pixel_min = 5000\n",
    "threshold = 0.5\n",
    "\n",
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up UNet model\n",
    "from model import UNet\n",
    "\n",
    "# instantiate & load weights\n",
    "seg_model = UNet(n_channels=3, n_classes=1, bilinear=False)\n",
    "seg_model.load_state_dict(torch.load(\n",
    "    'C:/Users/Fourth/models/unet_dice0.8369_save1746204838.pt',\n",
    "    map_location=device\n",
    "))\n",
    "seg_model.to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "def circular_crop(img, radius_ratio=0.4):\n",
    "    w, h = img.size\n",
    "    r = int(min(w, h) * radius_ratio)\n",
    "    cx, cy = w // 2, h // 2\n",
    "\n",
    "    mask = Image.new('L', (w, h), 0)\n",
    "    draw = ImageDraw.Draw(mask)\n",
    "    draw.ellipse((cx - r, cy - r, cx + r, cy + r), fill=255)\n",
    "\n",
    "    result = Image.new(\"RGB\", (w, h))\n",
    "    result.paste(img, mask=mask)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms as T\n",
    "import torchvision.transforms.functional as TF\n",
    "\n",
    "import torch\n",
    "\n",
    "class SegmentationPreprocessedDataset(Dataset):\n",
    "    def __init__(self, base_dataset, segmentation_model, device='cpu', threshold=threshold, transform=None):\n",
    "        self.base_dataset = base_dataset\n",
    "        self.segmentation_model = segmentation_model.to(device).eval()\n",
    "        self.device = device\n",
    "        self.threshold = threshold\n",
    "        self.transform = transform  # üëà Add this\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.base_dataset[idx]\n",
    "        original_image = TF.to_pil_image(image)\n",
    "\n",
    "        # Preprocess for UNet (e.g. resize to 300x300)\n",
    "        preprocess = T.Compose([\n",
    "            T.Resize((300, 300)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]) #this is a new line, i have not trained a new model with normalization yet\n",
    "        ])\n",
    "        input_tensor = preprocess(original_image).unsqueeze(0).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = self.segmentation_model(input_tensor)\n",
    "            mask = (output > self.threshold).float().squeeze(0)\n",
    "\n",
    "        # Resize mask to match original image resolution\n",
    "        mask = T.Resize(original_image.size[::-1])(mask)  # PIL size is (W, H), PyTorch is (H, W)\n",
    "\n",
    "        mask = mask.to(self.device)\n",
    "        \n",
    "        # ‚õëÔ∏è Safety mechanism: fallback if mask is too sparse\n",
    "        #try 1000 next?\n",
    "        min_pixel_threshold = pixel_min  # or use a ratio like 0.005 * mask.numel()\n",
    "        if mask.sum().item() < min_pixel_threshold:\n",
    "            # Fallback: either return original image or a circular crop\n",
    "            masked_pil = circular_crop(original_image)  # define circular_crop elsewhere\n",
    "        else:\n",
    "            # Apply mask\n",
    "            #masked_image = TF.to_tensor(original_image) * mask\n",
    "            masked_image = TF.to_tensor(original_image).to(self.device) * mask  # Move original image to the same device as mask\n",
    "            masked_pil = TF.to_pil_image(masked_image)\n",
    "\n",
    "        if self.transform:\n",
    "            masked_pil = self.transform(masked_pil)  # üëà Apply your original data_transform here\n",
    "\n",
    "        return masked_pil, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.base_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kyh8gLQwNQ0B",
    "outputId": "8e425d34-d7ac-455b-b3cf-85d438045745"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# Setup path to data folder (WindowsPath objects)\n",
    "data_path = Path(\"C:/Users/Fourth/data/\")\n",
    "image_path = data_path / \"melanoma_cancer_dataset/\"\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it...\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Download dataset\n",
    "    with open(data_path / \"Melanoma-dataset.zip\", \"wb\") as f:\n",
    "        request = requests.get(\"https://github.com/thereal4th/Melanoma-dataset/archive/refs/heads/main.zip\")\n",
    "        print(\"Downloading melanoma dataset...\")\n",
    "        f.write(request.content)\n",
    "\n",
    "    # Unzip zip file\n",
    "    with zipfile.ZipFile(data_path / \"Melanoma-dataset.zip\", \"r\") as zip_ref:\n",
    "        print(\"Unzipping data...\")\n",
    "        zip_ref.extractall(image_path)\n",
    "\n",
    "    # when Melanoma-dataset.zip is unzipped, it becomes a Melanoma-dataset-main directory\n",
    "    # Move train and test directories outside of zip folder to melanoma_cancer_dataset\n",
    "    train_dir = data_path/ \"melanoma_cancer_dataset/Melanoma-dataset-main/train\" # original train directory\n",
    "    test_dir = data_path/ \"melanoma_cancer_dataset/Melanoma-dataset-main/test\" # original test directory\n",
    "\n",
    "    # Move the entire directory\n",
    "    print(\"Moving train and test datasets...\")\n",
    "    shutil.move(str(train_dir), str(image_path / train_dir.name))\n",
    "    shutil.move(str(test_dir), str(image_path / test_dir.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QHdSkwX-QOd-",
    "outputId": "bd1017a8-29d1-4417-e7c0-977b2bc2d0c9"
   },
   "outputs": [],
   "source": [
    "# Setup new train and testing paths\n",
    "# these are WindowsPath objects\n",
    "train_dir = image_path / \"train\"\n",
    "test_dir = image_path / \"test\"\n",
    "\n",
    "train_dir, test_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dBANzLxSCdq",
    "outputId": "df3c72f2-3d32-48cf-d1c0-5bf3ee9f5a83"
   },
   "outputs": [],
   "source": [
    "#walk through the directory to view the image paths\n",
    "\n",
    "import os\n",
    "def walk_through_dir(dir_path):\n",
    "  \"\"\"\n",
    "  Walks through dir_path returning its contents.\n",
    "  Args:\n",
    "    dir_path (str or pathlib.Path): target directory\n",
    "\n",
    "  Returns:\n",
    "    A print out of:\n",
    "      number of subdiretories in dir_path\n",
    "      number of images (files) in each subdirectory\n",
    "      name of each subdirectory\n",
    "  \"\"\"\n",
    "  for dirpath, dirnames, filenames in os.walk(dir_path):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n",
    "\n",
    "#call the function\n",
    "walk_through_dir(image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TgexUF-Wc0wi",
    "outputId": "ef3d3cec-a9ea-45da-a8ad-54d340a9b9a2"
   },
   "outputs": [],
   "source": [
    "print(f\"Image path: {image_path}\")\n",
    "print(f\"Exists: {image_path.exists()}\")\n",
    "print(f\"Contents: {list(image_path.iterdir())}\")\n",
    "print(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386
    },
    "id": "ghVOJN4IieqU",
    "outputId": "eca3e451-88c7-491a-b85b-c289f0bf2374"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Set seed\n",
    "random.seed() # <- try changing this and see what happens\n",
    "\n",
    "# 1. Get all image paths (* means \"any combination\")\n",
    "image_path_list = list(image_path.glob(\"*/*/*.jpg\")) # array containing image paths\n",
    "\n",
    "# 2. Get random image path\n",
    "random_image_path = random.choice(image_path_list)\n",
    "\n",
    "# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\n",
    "image_class = random_image_path.parent.stem\n",
    "\n",
    "# 4. Open image\n",
    "img = Image.open(random_image_path) # displays the image\n",
    "\n",
    "# 5. Print metadata\n",
    "print(f\"Random image path: {random_image_path}\")\n",
    "print(f\"Image class: {image_class}\")\n",
    "print(f\"Image height: {img.height}\")\n",
    "print(f\"Image width: {img.width}\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 598
    },
    "id": "udhgGevSul7o",
    "outputId": "79f43a62-4c86-4ca7-9a48-ad482e842b67"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Turn the image into a numpy array\n",
    "img_as_array = np.asarray(img)\n",
    "#print(img_as_array)\n",
    "\n",
    "# Plot the image with matplotlib\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img_as_array)\n",
    "plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -> [height, width, color_channels]\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FQw3hXm0r-V"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7naTqHnS1S65"
   },
   "outputs": [],
   "source": [
    "# Simple transform pipeline using transforms.Compose\n",
    "to_tensor_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Write transform for image\n",
    "data_transform = transforms.Compose([\n",
    "    # crop the images to minimize black borders\n",
    "    # for some reason, cropping on some images increases the probability closer to 1 (malignant)\n",
    "    # transforms.CenterCrop((250, 250)),\n",
    "    # transform images to the value of imageResolution\n",
    "    transforms.Resize(size=(imageResolution, imageResolution)),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gmX6ie-H1TqX",
    "outputId": "025276f7-d802-469a-8d4f-e033c59fa8ee"
   },
   "outputs": [],
   "source": [
    "# not used for training, only for visualization\n",
    "def plot_transformed_images(image_paths, transform, n=3, seed=42):\n",
    "    \"\"\"Plots a series of random images from image_paths.\n",
    "\n",
    "    Will open n image paths from image_paths, transform them\n",
    "    with transform and plot them side by side.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of target image paths.\n",
    "        transform (PyTorch Transforms): Transforms to apply to images.\n",
    "        n (int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    #random.seed(seed)\n",
    "    random.seed()\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f: #the image.open() method takes in an image path and opens the image.\n",
    "            fig, ax = plt.subplots(1, 2) # 1 row and 2 columns\n",
    "            ax[0].imshow(f)\n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            # Note: permute() will change shape of image to suit matplotlib\n",
    "            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n",
    "            transformed_image = transform(f).permute(1, 2, 0)\n",
    "            ax[1].imshow(transformed_image)\n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16) # .parent attribute refers to parent directories and .stem takes the name of the directories (benign or malignant)\n",
    "\n",
    "plot_transformed_images(image_path_list,\n",
    "                        transform=data_transform,\n",
    "                        n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m-hfGVdm1VGF",
    "outputId": "9afb49d0-0b4a-43f2-8de0-8e9391bd3a05"
   },
   "outputs": [],
   "source": [
    "# Use ImageFolder class from datasets module from torchvision package to create dataset(s)\n",
    "# ImageFolder assumes a proper structure of directories for your dataset in order to read the classes (labels) correctly.\n",
    "from torchvision import datasets #import datasets module from the torchvision package\n",
    "#                               (a module is just a python file that contains classes and such, datasets.py is a module)\n",
    "#                                a package is just a directory that organizes modules.\n",
    "# construct ImageFolder object \n",
    "'''train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
    "                                  transform=data_transform, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                 transform=data_transform)'''\n",
    "\n",
    "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
    "                                  transform=to_tensor_transform, # transforms to perform on data (images)\n",
    "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
    "\n",
    "test_data = datasets.ImageFolder(root=test_dir,\n",
    "                                 transform=to_tensor_transform)\n",
    "\n",
    "# Wrap datasets and pass in transform\n",
    "segmented_train_data = SegmentationPreprocessedDataset(train_data, seg_model, device=device, transform=data_transform)\n",
    "segmented_test_data = SegmentationPreprocessedDataset(test_data, seg_model, device=device, transform=data_transform)\n",
    "\n",
    "# Wrap datasets and pass in transform\n",
    "segmented_train_data = SegmentationPreprocessedDataset(train_data, seg_model, device=device, transform=data_transform)\n",
    "segmented_test_data = SegmentationPreprocessedDataset(test_data, seg_model, device=device, transform=data_transform)\n",
    "\n",
    "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65KCBGlnKv-Q",
    "outputId": "18b27e11-09b3-4d09-d8ea-b730582f9f05"
   },
   "outputs": [],
   "source": [
    "# Get class names as a list\n",
    "# .classes comes with ImageFolder class\n",
    "class_names = train_data.classes\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e2vEM5ooKycT",
    "outputId": "a49bdfa0-96fa-4d21-d599-4178a8493db6"
   },
   "outputs": [],
   "source": [
    "# Can also get class names as a dict\n",
    "class_dict = train_data.class_to_idx\n",
    "class_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l5CWKJWbK24f",
    "outputId": "d0997ab4-7003-4dfa-e566-62e67c900451"
   },
   "outputs": [],
   "source": [
    "# Check the lengths\n",
    "len(train_data), len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qgrx_U2lK_hk",
    "outputId": "648e894f-c817-4276-92c2-c5d2081924ed"
   },
   "outputs": [],
   "source": [
    "#for visualization, check the first image and its label in the train dataset\n",
    "\n",
    "img, label = segmented_train_data[0][0], segmented_train_data[0][1]\n",
    "\n",
    "#the image tensors are in the first column and the labels are in the 2nd column\n",
    "#the labels are labeled as numbers, 0 for benign and 1 for malignant. (i think this depends on the order of your folders in your folder structure)\n",
    "\n",
    "print(f\"Image tensor:\\n{img}\")\n",
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label: {label}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 634
    },
    "id": "uiPhj7bnLQS9",
    "outputId": "b4eb591f-d7e0-4077-e7df-7e389bf261c0"
   },
   "outputs": [],
   "source": [
    "# Rearrange the order of dimensions\n",
    "# not really used anywhere in the program, just demonstrated.\n",
    "img_permute = img.permute(1, 2, 0) #originally (0, 1, 2) or (3, 224, 224)\n",
    "\n",
    "# Print out different shapes (before and after permute)\n",
    "print(f\"Original shape: {img.shape} -> [color_channels, height, width]\")\n",
    "print(f\"Image permute shape: {img_permute.shape} -> [height, width, color_channels]\")\n",
    "\n",
    "# Plot the image\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(class_names[label], fontsize=14);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yrDW4LG4RU-n",
    "outputId": "f4272137-c03b-4073-c17d-38dd15216ef0"
   },
   "outputs": [],
   "source": [
    "# Turn train and test Datasets into DataLoaders\n",
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(dataset=segmented_train_data, # train_data is the dataset object created from the ImageFolder class using the train_dir directory from PathLib\n",
    "                              batch_size=batchSize, # how many samples per batch?\n",
    "                              num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n",
    "                              shuffle=True) # shuffle the data?\n",
    "\n",
    "test_dataloader = DataLoader(dataset=segmented_test_data, # test_data is the dataset object created from the ImageFolder class using the test_dir directory from PathLib\n",
    "                             batch_size=batchSize,\n",
    "                             num_workers=0,\n",
    "                             shuffle=False) # don't usually need to shuffle testing data\n",
    "\n",
    "train_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1Jf_O8MT70_",
    "outputId": "18471b8b-1fa4-4fef-f4cb-7fdff0650e03"
   },
   "outputs": [],
   "source": [
    "img, label = next(iter(train_dataloader))\n",
    "# img contains image tensors\n",
    "# label contains the classess, 0 or 1 for benign or malignant.\n",
    "\n",
    "# Batch size will now be 16, try changing the batch_size parameter above and see what happens\n",
    "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
    "print(f\"Label shape: {label.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QdA3svMGi2pC"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from typing import Tuple, Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8QL8j5ugcWg"
   },
   "outputs": [],
   "source": [
    "# Make function to find classes in target directory\n",
    "# helper function\n",
    "def find_classes(directory: str) -> Tuple[List[str], Dict[str, int]]:\n",
    "    \"\"\"Finds the class folder names in a target directory.\n",
    "\n",
    "    Assumes target directory is in standard image classification format.\n",
    "\n",
    "    Args:\n",
    "        directory (str): target directory to load classnames from.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n",
    "\n",
    "    Example:\n",
    "        find_classes(\"food_images/train\")\n",
    "        >>> ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n",
    "    \"\"\"\n",
    "    # 1. Get the class names by scanning the target directory\n",
    "    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n",
    "\n",
    "    # 2. Raise an error if class names not found\n",
    "    if not classes:\n",
    "        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n",
    "\n",
    "    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hr6UauaeeGzU",
    "outputId": "1288b2f0-3196-40f0-f126-a56c6e60f994"
   },
   "outputs": [],
   "source": [
    "#print classes\n",
    "\n",
    "find_classes(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XHvgH4aoqjZ"
   },
   "source": [
    "EXPERIMENTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeQMp_93ovHy"
   },
   "outputs": [],
   "source": [
    "# 1. Take in a Dataset as well as a list of class names\n",
    "def display_random_images(dataset: torch.utils.data.dataset.Dataset,\n",
    "                          classes: List[str] = None,\n",
    "                          n: int = 10,\n",
    "                          display_shape: bool = True,\n",
    "                          seed: int = None):\n",
    "\n",
    "    # 2. Adjust display if n too high\n",
    "    if n > 10:\n",
    "        n = 10\n",
    "        display_shape = False\n",
    "        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n",
    "\n",
    "    # 3. Set random seed\n",
    "    if seed:\n",
    "        random.seed(seed)\n",
    "\n",
    "    # 4. Get random sample indexes\n",
    "    random_samples_idx = random.sample(range(len(dataset)), k=n)\n",
    "\n",
    "    # 5. Setup plot\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # 6. Loop through samples and display random samples\n",
    "    for i, targ_sample in enumerate(random_samples_idx):\n",
    "        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n",
    "\n",
    "        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -> [color_channels, height, width]\n",
    "        targ_image_adjust = targ_image.permute(1, 2, 0)\n",
    "\n",
    "        # Plot adjusted samples\n",
    "        plt.subplot(1, n, i+1)\n",
    "        plt.imshow(targ_image_adjust)\n",
    "        plt.axis(\"off\")\n",
    "        if classes:\n",
    "            title = f\"class: {classes[targ_label]}\"\n",
    "            if display_shape:\n",
    "                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "oOiSs_0Jtlli",
    "outputId": "5a00e7ca-78e5-4efb-f7c8-9d29dea35cb4"
   },
   "outputs": [],
   "source": [
    "# Display random images from ImageFolder created Dataset\n",
    "display_random_images(segmented_train_data,\n",
    "                      n=5,\n",
    "                      classes=class_names,\n",
    "                      seed=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TCF-Ia4ZF4oz",
    "outputId": "7d7b0855-bb7d-4127-c9e9-1def194e3162"
   },
   "outputs": [],
   "source": [
    "  class MelanomaClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Model architecture TinyVGG from:\n",
    "    https://poloclub.github.io/cnn-explainer/\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3, # how big is the square that's going over the image?\n",
    "                      stride=1, # default\n",
    "                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units,\n",
    "                      out_channels=hidden_units,\n",
    "                      kernel_size=3,\n",
    "                      stride=1,\n",
    "                      padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2,\n",
    "                          stride=2) # default stride value is same as kernel_size\n",
    "\n",
    "        )\n",
    "\n",
    "        hidden_units_B = hidden_units * 2 #increase out-feautures hidden units (32 to 64)\n",
    "        print(hidden_units_B) # debug\n",
    "\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units_B, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units_B, hidden_units_B, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        hidden_units = hidden_units_B * 2 #increase out-feautures hidden units_B (64 to 128)\n",
    "        print(hidden_units) # debug\n",
    "\n",
    "        self.conv_block_3 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units_B, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        hidden_units_B = hidden_units * 2 #ncrease out-feautures hidden units (128 to 256)\n",
    "        print(hidden_units_B) # debug\n",
    "        \n",
    "        self.conv_block_4 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units_B, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units_B, hidden_units_B, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        \n",
    "        self._to_linear = None\n",
    "        self._compute_linear_input_size(input_shape) #dynamically compute in_features\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Where did this in_features shape come from?\n",
    "            # It's because each layer of our network compresses and changes the shape of our input data.\n",
    "            nn.Linear(in_features=self._to_linear,\n",
    "                      out_features=output_shape)\n",
    "\n",
    "        )\n",
    "\n",
    "    # Function to automate the calculation of in_features for the final nn.linear layer by passing a dummy tensor into the model.\n",
    "    def _compute_linear_input_size(self, input_shape):\n",
    "        \"\"\"Pass a dummy tensor through conv layers by making an inference to determine in_features size.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.randn(1, input_shape, imageResolution, imageResolution)  # Dummy tensor input to simulate the changes of the in_features as a tensor passes the conv blocks.\n",
    "            output = self.conv_block_1(dummy_input)\n",
    "            output = self.conv_block_2(output)\n",
    "            output = self.conv_block_3(output)\n",
    "            output = self.conv_block_4(output)\n",
    "            self._to_linear = output.view(1, -1).shape[1]  # Flatten and get feature size\n",
    "            print(\"Computed in_features:\", self._to_linear)  # Debugging\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_2(x)\n",
    "        # print(x.shape)\n",
    "        x = self.conv_block_3(x)\n",
    "        x = self.conv_block_4(x)\n",
    "        print(f\"Feature map shape before flattening: {x.shape}\")  # Debugging line\n",
    "        x = self.classifier(x)\n",
    "        # print(x.shape)\n",
    "        return x\n",
    "        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # <- leverage the benefits of operator fusion\n",
    "\n",
    "  #create model instance\n",
    "  torch.manual_seed(42)\n",
    "  model_0 = MelanomaClassifier(input_shape=3, # number of color channels (3 for RGB)\n",
    "                    hidden_units=32, #initial hidden units\n",
    "                    output_shape=1).to(device)\n",
    "  model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T8xvpoySGw9G",
    "outputId": "4466736d-c6f2-4d19-be8e-ee5eaf1e3178"
   },
   "outputs": [],
   "source": [
    "  # 1. Get a batch of images and labels from the DataLoader\n",
    "  img_batch, label_batch = next(iter(train_dataloader))\n",
    "\n",
    "  # 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
    "  img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
    "  print(f\"Single image shape: {img_single.shape}\\n\")\n",
    "\n",
    "  # 3. Perform a forward pass on a single image\n",
    "  model_0.eval()\n",
    "  with torch.inference_mode():\n",
    "      pred = model_0(img_single.to(device))\n",
    "\n",
    "  # 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
    "  print(f\"Output logits:\\n{pred}\\n\")\n",
    "  print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
    "  print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
    "  print(f\"Actual label:\\n{label_single}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbj4QAfaJnMY",
    "outputId": "86698cb4-3728-4590-edcf-cb91fa84f7f3"
   },
   "outputs": [],
   "source": [
    "# Install torchinfo if it's not available, import it if it is\n",
    "try:\n",
    "    import torchinfo\n",
    "except:\n",
    "    !pip install torchinfo\n",
    "    import torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model_0, input_size=[1, 3, imageResolution, imageResolution]) # do a test pass through of an example input size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OU9pTYPWTytj"
   },
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               loss_fn: torch.nn.Module,\n",
    "               optimizer: torch.optim.Optimizer):\n",
    "    # Put model in train mode\n",
    "    model.train()\n",
    "\n",
    "    # Setup train loss and train accuracy values\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    # Loop through data loader data batches\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 1. Forward pass\n",
    "        y_pred = model(X)\n",
    "\n",
    "        # 2. Calculate  and accumulate loss\n",
    "        #loss = loss_fn(y_pred, y)\n",
    "        loss = loss_fn(y_pred, y.unsqueeze(1).type(torch.float32))\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate and accumulate accuracy metrics across all batches\n",
    "        #Convert logits to probabilities for binary classification\n",
    "        y_pred_probs = torch.sigmoid(y_pred)\n",
    "        y_pred_class = (y_pred_probs > 0.5).float()  # Convert to 0 or 1\n",
    "\n",
    "        train_acc += (y_pred_class == y.unsqueeze(1)).sum().item() #originally train_acc += (y_pred_class == y.unsqueeze(1)).sum().item()\n",
    "        #gpt suggests `train_acc += ((y_pred_class == y.unsqueeze(1)).sum().item() / y.size(0))\n",
    "        print(f\"Output shape: {y_pred.shape}\")  # Should be (batch_size, 1)\n",
    "        print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss = train_loss / len(dataloader.dataset) #originally `len(dataloader)` but chatgpt told me to change it\n",
    "    train_acc = train_acc / len(dataloader.dataset) #originally `len(dataloader)` but chatgpt told me to change it\n",
    "    return train_loss, train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AYO4LhT0T0DT"
   },
   "outputs": [],
   "source": [
    "  def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              loss_fn: torch.nn.Module):\n",
    "    # Put model in eval mode\n",
    "    model.eval()\n",
    "\n",
    "    # Setup test loss and test accuracy values\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    # Turn on inference context manager\n",
    "    with torch.inference_mode():\n",
    "        # Loop through DataLoader batches\n",
    "        for batch, (X, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            # 1. Forward pass\n",
    "            test_pred_logits = model(X)\n",
    "\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(test_pred_logits, y.unsqueeze(1).type(torch.float32))\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # Calculate and accumulate accuracy\n",
    "            test_pred_probs = torch.sigmoid(test_pred_logits)  # Convert logits to probabilities\n",
    "            test_pred_labels = (test_pred_probs > 0.5).float()  # Convert probabilities to binary labels\n",
    "\n",
    "            # originally `test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))` but gpt suggested this:\n",
    "            test_acc += (test_pred_labels == y.unsqueeze(1)).sum().item()\n",
    "\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    test_loss = test_loss / len(dataloader.dataset) #originally `len(dataloader)` but chatgpt told me to change it\n",
    "    test_acc = test_acc / len(dataloader.dataset) #originally `len(dataloader)` but chatgpt told me to change it\n",
    "    return test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IvU3J_IMT8Kw"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Take in various parameters required for training and test steps\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module = nn.BCEWithLogitsLoss(),\n",
    "          epochs: int = 5):\n",
    "\n",
    "    best_acc = 0.0\n",
    "\n",
    "    #SAVING THE MODEL PARAMETERS\n",
    "    # 1. Create models directory\n",
    "    MODEL_PATH = Path(\"saved models\")\n",
    "    MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 2. Create empty results dictionary\n",
    "    results = {\"train_loss\": [],\n",
    "        \"train_acc\": [],\n",
    "        \"test_loss\": [],\n",
    "        \"test_acc\": []\n",
    "    }\n",
    "\n",
    "    # 3. Loop through training and testing steps for a number of epochs\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_acc = train_step(model=model,\n",
    "                                           dataloader=train_dataloader,\n",
    "                                           loss_fn=loss_fn,\n",
    "                                           optimizer=optimizer)\n",
    "        test_loss, test_acc = test_step(model=model,\n",
    "            dataloader=test_dataloader,\n",
    "            loss_fn=loss_fn)\n",
    "\n",
    "        # 4. Print out what's happening\n",
    "        print(\n",
    "            f\"Epoch: {epoch+1} | \"\n",
    "            f\"train_loss: {train_loss:.4f} | \"\n",
    "            f\"train_acc: {train_acc:.4f} | \"\n",
    "            f\"test_loss: {test_loss:.4f} | \"\n",
    "            f\"test_acc: {test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "        # 5. Update results dictionary\n",
    "        # Ensure all data is moved to CPU and converted to float for storage\n",
    "        results[\"train_loss\"].append(train_loss.item() if isinstance(train_loss, torch.Tensor) else train_loss)\n",
    "        results[\"train_acc\"].append(train_acc.item() if isinstance(train_acc, torch.Tensor) else train_acc)\n",
    "        results[\"test_loss\"].append(test_loss.item() if isinstance(test_loss, torch.Tensor) else test_loss)\n",
    "        results[\"test_acc\"].append(test_acc.item() if isinstance(test_acc, torch.Tensor) else test_acc)\n",
    "\n",
    "        # Create model save path\n",
    "        MODEL_NAME = f\"model_with-UNET_epoch{epoch+1}_acc{test_acc:.4f}_thres{threshold}_minpixel{pixel_min}_batch{batchSize}_normalized.pth\"\n",
    "        MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "        # Check if current epoch has the best acc\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "        \n",
    "            # Save the model state dict\n",
    "            print(f\"Saving model to: {MODEL_SAVE_PATH}\")\n",
    "            checkpoint = {\n",
    "                \"model_state_dict\": model_0.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            }\n",
    "            torch.save(checkpoint, MODEL_SAVE_PATH)\n",
    "\n",
    "    # 6. Return the filled results at the end of the epochs\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "9d58e6f66a924b5390a1a99d2f09e7ba",
      "62fa884908534e04b3518702612a5d5e",
      "baa3a3062bc8483e8e71cbde9130c19d",
      "e680a38ea5414769bf9e0916df590e4f",
      "6dad1035ef734430951d443417577a23",
      "d11aa60aa8e84814b3c5819ad1301ab9",
      "dc5a2276570b46968723da767b3d2401",
      "138ebf53134246689b2646e03360b9d7",
      "80654542695a451184a5c22e1ddb15e9",
      "385ccade65ce44078963f18ae43e7895",
      "f1b82175fb444ca79819989ed6b41cca"
     ]
    },
    "id": "Ark2r_WkVt8y",
    "outputId": "335d1741-2fed-4a58-fc13-34c94b2d4b0d"
   },
   "outputs": [],
   "source": [
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set number of epochs\n",
    "NUM_EPOCHS = 20 #20 seems to be the max\n",
    "\n",
    "# Recreate an instance of TinyVGG\n",
    "model_0 = MelanomaClassifier(input_shape=3, # number of color channels (3 for RGB)\n",
    "                  hidden_units=10,\n",
    "                  output_shape=1).to(device)\n",
    "\n",
    "# Setup loss function and optimizer\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer\n",
    "start_time = timer()\n",
    "\n",
    "# Train model_0\n",
    "model_0_results = train(model=model_0,\n",
    "                        train_dataloader=train_dataloader,\n",
    "                        test_dataloader=test_dataloader,\n",
    "                        optimizer=optimizer,\n",
    "                        loss_fn=loss_fn,\n",
    "                        epochs=NUM_EPOCHS)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"Total training time: {end_time-start_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GrKFlsY-cgP5"
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(results: Dict[str, List[float]]):\n",
    "\n",
    "    \"\"\"Plots training curves of a results dictionary.\n",
    "\n",
    "    Args:\n",
    "        results (dict): dictionary containing list of values, e.g.\n",
    "            {\"train_loss\": [...],\n",
    "             \"train_acc\": [...],\n",
    "             \"test_loss\": [...],\n",
    "             \"test_acc\": [...]}\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pe5Q_fxOcn_o"
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(model_0_results)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "138ebf53134246689b2646e03360b9d7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "385ccade65ce44078963f18ae43e7895": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62fa884908534e04b3518702612a5d5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d11aa60aa8e84814b3c5819ad1301ab9",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_dc5a2276570b46968723da767b3d2401",
      "value": "‚Äá‚Äá0%"
     }
    },
    "6dad1035ef734430951d443417577a23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "80654542695a451184a5c22e1ddb15e9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d58e6f66a924b5390a1a99d2f09e7ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_62fa884908534e04b3518702612a5d5e",
       "IPY_MODEL_baa3a3062bc8483e8e71cbde9130c19d",
       "IPY_MODEL_e680a38ea5414769bf9e0916df590e4f"
      ],
      "layout": "IPY_MODEL_6dad1035ef734430951d443417577a23"
     }
    },
    "baa3a3062bc8483e8e71cbde9130c19d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_138ebf53134246689b2646e03360b9d7",
      "max": 20,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_80654542695a451184a5c22e1ddb15e9",
      "value": 0
     }
    },
    "d11aa60aa8e84814b3c5819ad1301ab9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc5a2276570b46968723da767b3d2401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e680a38ea5414769bf9e0916df590e4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_385ccade65ce44078963f18ae43e7895",
      "placeholder": "‚Äã",
      "style": "IPY_MODEL_f1b82175fb444ca79819989ed6b41cca",
      "value": "‚Äá0/20‚Äá[00:00&lt;?,‚Äá?it/s]"
     }
    },
    "f1b82175fb444ca79819989ed6b41cca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
